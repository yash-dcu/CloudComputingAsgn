1.) adding words.csv to hdfs
hdfs dfs -put words.csv /all_data


2.) create and load data into words table
create table words (words string) row format delimited fields terminated by ',' lines terminated by '\n' tblproperties ('skip.header.line.count'='1');
load data inpath '/all_data/words.csv' into table words.csv


3.) Add spam_label column to dataset and set default value as 0
alter table dataset add columns (spam_label int);
update dataset set spam_label=0;


4.) update spam_label values in dataset 
update dataset set spam_label=1 from dataset full join words on dataset.body rlike words.words;


5.) create spam table and transfer data from dataset to this table
create table spam_dataset (sender_id string, date_time string, subject string, body string)
clustered by (date_time) into 4 buckets
stored as orc
tblproperties ('transactional'='true');

insert into spam_dataset select sender_id, date_time, subject, body from dataset;


6.) create ham table
create table ham_dataset (sender_id string, date_time string, subject string, body string)
clustered by (date_time) into 4 buckets
stored as orc
tblproperties ('transactional'='true');

insert into ham_dataset select sender_id, date_time, subject, body from dataset;


7.) top 10 spam account
select sender_id , count(sender_id) as counter from spam_dataset group by sender_id order by counter desc limit 10;


8.) top 10 ham account
select sender_id , count(sender_id) as counter from ham_dataset group by sender_id order by counter desc limit 10;
